{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <center> Sarcasm detection with logistic regression. Solution\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. ","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"}},{"cell_type":"code","source":"PATH_TO_DATA = '../input/sarcasm/train-balanced-sarcasm.csv'","metadata":{"_uuid":"ed87ab2845921166bb73ca854bfe1ef013c035e9","execution":{"iopub.status.busy":"2021-09-14T10:42:01.015334Z","iopub.execute_input":"2021-09-14T10:42:01.015658Z","iopub.status.idle":"2021-09-14T10:42:01.035412Z","shell.execute_reply.started":"2021-09-14T10:42:01.015606Z","shell.execute_reply":"2021-09-14T10:42:01.034776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.status.busy":"2021-09-14T10:42:01.036729Z","iopub.execute_input":"2021-09-14T10:42:01.03716Z","iopub.status.idle":"2021-09-14T10:42:02.455518Z","shell.execute_reply.started":"2021-09-14T10:42:01.037118Z","shell.execute_reply":"2021-09-14T10:42:02.454273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(PATH_TO_DATA)","metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.status.busy":"2021-09-14T10:42:02.457044Z","iopub.execute_input":"2021-09-14T10:42:02.457377Z","iopub.status.idle":"2021-09-14T10:42:10.915969Z","shell.execute_reply.started":"2021-09-14T10:42:02.457318Z","shell.execute_reply":"2021-09-14T10:42:10.915094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.status.busy":"2021-09-14T10:42:10.917619Z","iopub.execute_input":"2021-09-14T10:42:10.917876Z","iopub.status.idle":"2021-09-14T10:42:10.966338Z","shell.execute_reply.started":"2021-09-14T10:42:10.917831Z","shell.execute_reply":"2021-09-14T10:42:10.965243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.status.busy":"2021-09-14T10:42:10.967654Z","iopub.execute_input":"2021-09-14T10:42:10.967948Z","iopub.status.idle":"2021-09-14T10:42:11.738824Z","shell.execute_reply.started":"2021-09-14T10:42:10.967864Z","shell.execute_reply":"2021-09-14T10:42:11.738078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows.","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"}},{"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.status.busy":"2021-09-14T10:42:22.271294Z","iopub.execute_input":"2021-09-14T10:42:22.271652Z","iopub.status.idle":"2021-09-14T10:42:22.604307Z","shell.execute_reply.started":"2021-09-14T10:42:22.271567Z","shell.execute_reply":"2021-09-14T10:42:22.60341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that the dataset is indeed balanced","metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"}},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11","execution":{"iopub.status.busy":"2021-09-14T10:42:40.732295Z","iopub.execute_input":"2021-09-14T10:42:40.732631Z","iopub.status.idle":"2021-09-14T10:42:40.752677Z","shell.execute_reply.started":"2021-09-14T10:42:40.73257Z","shell.execute_reply":"2021-09-14T10:42:40.751689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split data into training and validation parts.","metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"}},{"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","metadata":{"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96","execution":{"iopub.status.busy":"2021-09-14T10:42:49.155513Z","iopub.execute_input":"2021-09-14T10:42:49.155847Z","iopub.status.idle":"2021-09-14T10:42:49.434248Z","shell.execute_reply.started":"2021-09-14T10:42:49.155797Z","shell.execute_reply":"2021-09-14T10:42:49.433152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.","metadata":{"_uuid":"8ccf65310c5e61194dd6a8913276e54cc32e7712"}},{"cell_type":"markdown","source":"### Part 1. Exploratory data analysis","metadata":{"_uuid":"ba1a8f65032c5954476a68e01b607655145b746d"}},{"cell_type":"markdown","source":"Distribution of lengths for sarcastic and normal comments is almost the same.","metadata":{"_uuid":"6a045e347fabd462a643639a1334b51f8780627d"}},{"cell_type":"code","source":"train_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5)\ntrain_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).hist(label='normal', alpha=.5)\nplt.legend();","metadata":{"_uuid":"dadaf341602993a7854867a1df3004d0aa5d9b8c","execution":{"iopub.status.busy":"2021-09-14T10:42:59.701137Z","iopub.execute_input":"2021-09-14T10:42:59.701443Z","iopub.status.idle":"2021-09-14T10:43:00.777802Z","shell.execute_reply.started":"2021-09-14T10:42:59.701397Z","shell.execute_reply":"2021-09-14T10:43:00.776737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS","metadata":{"_uuid":"c2c613ee2052a2c0379682adf5c23d1f751f4c3b","execution":{"iopub.status.busy":"2021-09-14T10:44:03.663711Z","iopub.execute_input":"2021-09-14T10:44:03.664173Z","iopub.status.idle":"2021-09-14T10:44:03.688397Z","shell.execute_reply.started":"2021-09-14T10:44:03.66413Z","shell.execute_reply":"2021-09-14T10:44:03.687464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)","metadata":{"_uuid":"ae7333d67f6a17673d2aa16aed3017e2fbef9b58","execution":{"iopub.status.busy":"2021-09-14T10:44:04.511796Z","iopub.execute_input":"2021-09-14T10:44:04.512104Z","iopub.status.idle":"2021-09-14T10:44:04.517667Z","shell.execute_reply.started":"2021-09-14T10:44:04.512057Z","shell.execute_reply":"2021-09-14T10:44:04.515745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word cloud are nice, but not very useful","metadata":{"_uuid":"7a59f35dc359cb1b13a363b0f515b38a03c7b940"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 1, 'comment']))\nplt.imshow(wordcloud);","metadata":{"_uuid":"7bc095d9b0c549d4a21478ed52845210c0ffbb57","execution":{"iopub.status.busy":"2021-09-14T10:44:08.240128Z","iopub.execute_input":"2021-09-14T10:44:08.240657Z","iopub.status.idle":"2021-09-14T10:44:09.537842Z","shell.execute_reply.started":"2021-09-14T10:44:08.240374Z","shell.execute_reply":"2021-09-14T10:44:09.537035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 0, 'comment']))\nplt.imshow(wordcloud);","metadata":{"_uuid":"f2423d8c8adf818c0f7c709665f741e1f885e47f","execution":{"iopub.status.busy":"2021-09-14T10:44:11.961044Z","iopub.execute_input":"2021-09-14T10:44:11.961332Z","iopub.status.idle":"2021-09-14T10:44:13.183953Z","shell.execute_reply.started":"2021-09-14T10:44:11.961293Z","shell.execute_reply":"2021-09-14T10:44:13.183258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's analyze whether some subreddits are more \"sarcastic\" on average than others","metadata":{"_uuid":"15c4140c01495b4d62a5b18d1906ba191e01505c"}},{"cell_type":"code","source":"sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='sum', ascending=False).head(10)","metadata":{"_uuid":"6ba84720d54144e054b6963d78b48bf648e5c652","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)","metadata":{"_uuid":"575136be3a080cdd9a93ee7ee0c5fc9d9bf754e9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same for authors doesn't yield much insight. Except for the fact that somebody's comments were sampled - we can see the same amounts of sarcastic and non-sarcastic comments.","metadata":{"_uuid":"4e9cf503955b00599953ef8f87d0662155876519"}},{"cell_type":"code","source":"sub_df = train_df.groupby('author')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","metadata":{"_uuid":"1cde1056ef0d3c62b98d70a299d83df29fcf6071","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = train_df[train_df['score'] >= 0].groupby('score')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","metadata":{"_uuid":"46f4aa1c9524d525acec04ca0754f85a5190514b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = train_df[train_df['score'] < 0].groupby('score')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","metadata":{"_uuid":"3e2f474c6219798c2fef2e4004ad4a6c1022f8ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 2. Training the model","metadata":{"_uuid":"416321f19f5a27290bc5622e8b3384b7bbbd28c6"}},{"cell_type":"code","source":"# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=1)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","metadata":{"_uuid":"3048a070a56b08eb4e5fe2c54b6d14905031e74a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntfidf_logit_pipeline.fit(train_texts, y_train)","metadata":{"_uuid":"8756bac7457218e4daf08ec276211f03971c17fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)","metadata":{"_uuid":"d2e47f77f999c2fb5aee9ef1de1542bc93de4c98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_valid, valid_pred)","metadata":{"_uuid":"a8f93efc3db12910eaa6d7944feebb2418714203","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 3. Explaining the model","metadata":{"_uuid":"fefd0178f43ce832031653be70f0a0e47f62cf4c"}},{"cell_type":"code","source":"def plot_confusion_matrix(actual, predicted, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(actual, predicted).T\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","metadata":{"_uuid":"247a13fd3ae4d5c015c0ca0489a9a95d72ad7e9f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion matrix is quite balanced.","metadata":{"_uuid":"ff4a79b0368176a518fb0b84b45a508499e6183f"}},{"cell_type":"code","source":"plot_confusion_matrix(y_valid, valid_pred, \n                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))","metadata":{"_uuid":"6df0c058a45b48b756e57e01a23bbc0974407195","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we can recognize some phrases indicative of sarcasm. Like \"yes sure\". ","metadata":{"_uuid":"6af3a7c93afef23ce9d215bf1daa2c91feb57d5d"}},{"cell_type":"code","source":"import eli5\neli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","metadata":{"_uuid":"f62f3043b6e94fb6bbd5683a0e9662c572847fa6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So sarcasm detection is easy.\n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />","metadata":{"_uuid":"be94f2065f86c65d5ee5590c9b2e5a541135732c"}},{"cell_type":"markdown","source":"### Part 4. Improving the model","metadata":{"_uuid":"5648f6ad7a14ef3a582909f7c0c72c4fc80204aa"}},{"cell_type":"code","source":"subreddits = train_df['subreddit']\ntrain_subreddits, valid_subreddits = train_test_split(subreddits, random_state=17)","metadata":{"_uuid":"aaefd2eb6f829f9eb9e9cd12c7903d3086182acc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll have separate Tf-Idf vectorizers for comments and for subreddits. It's possible to stick to a pipeline as well, but in that case it becomes a bit less straightforward. [Example](https://stackoverflow.com/questions/36731813/computing-separate-tfidf-scores-for-two-different-columns-using-sklearn)","metadata":{"_uuid":"4a0dac5edc6b5c4078622637454baa820bc18a5f"}},{"cell_type":"code","source":"tf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\ntf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))","metadata":{"_uuid":"88be690ae260d824fbd8df4a4d02e5abcce0d5a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do transformations separately for comments and subreddits. ","metadata":{"_uuid":"453f8bc16c7726cbff936f3170442341eca3b45e"}},{"cell_type":"code","source":"%%time\nX_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)","metadata":{"_uuid":"bfd35a513a3a090485df667e8ec773c57682aae7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_texts.shape, X_valid_texts.shape","metadata":{"_uuid":"bea0b7a09c57afefb77e5eabd2b8ed18bf019855","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)\nX_valid_subreddits = tf_idf_subreddits.transform(valid_subreddits)","metadata":{"_uuid":"b81a4ad703e2fb0f97f21a449af7e0d8b39a860c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_subreddits.shape, X_valid_subreddits.shape","metadata":{"_uuid":"d55aec9753a506f41310b4b717cdbb8694af8a8e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, stack all features together.","metadata":{"_uuid":"74b044c4b73653181ddde0f32e65a14d4867319c"}},{"cell_type":"code","source":"from scipy.sparse import hstack\nX_train = hstack([X_train_texts, X_train_subreddits])\nX_valid = hstack([X_valid_texts, X_valid_subreddits])","metadata":{"_uuid":"bc458646e3c36792cf845f86cb9b6d6cc384b32c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_valid.shape","metadata":{"_uuid":"d5d2505567c9303b2ce9f81c9bdc11fc799af91e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the same logistic regression.","metadata":{"_uuid":"a7f2040b76acbf2ff58e5dc68d7437ee6c3e9989"}},{"cell_type":"code","source":"logit.fit(X_train, y_train)","metadata":{"_uuid":"77b08444a449def113803365001d1f844620d5aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvalid_pred = logit.predict(X_valid)","metadata":{"_uuid":"1a48a2e637db7e695766af5be6f5aa60596f6ad9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_valid, valid_pred)","metadata":{"_uuid":"b50af0bc6ea1297538daaff183b7eecf8dfa7c2c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, accuracy slightly increased.","metadata":{"_uuid":"5fe6b371e91831198dda10d8b198c9e1ebfe07aa"}}]}